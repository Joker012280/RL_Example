{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "## CartPole-v0 환경을 이용, 폴이 안쓰러지게 학습\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training 중인 상태를 plot으로 표현\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.axhline(y=200,linestyle ='--')\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experience Replay\n",
    "## 학습에 이용할 메모리\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(transition)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        if len(self.memory) >= self.capacity: \n",
    "            self.memory[self.position] = transition\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 큰 차원의 데이터를 다루기위해서 사용\n",
    "\n",
    "\n",
    "## 학습에 쓰이는  NN \n",
    "## Input은 State가 되고 Output은 Q - value가 된다.\n",
    "## 마지막에서 4개의 Q - Value를 Output으로 가진다. \n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size*2)\n",
    "        self.fc3 = nn.Linear(hidden_size*2,action_size)\n",
    "        self.fc4 = nn.Linear(hidden_size*2,action_size)\n",
    "        self.fc5 = nn.Linear(hidden_size*2,action_size)\n",
    "        self.fc6 = nn.Linear(hidden_size*2,action_size)\n",
    "        \n",
    "    ## 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됨.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x1 = self.fc3(x)\n",
    "        x2 = self.fc4(x)\n",
    "        x3 = self.fc5(x)\n",
    "        x4 = self.fc6(x)\n",
    "        return x1,x2,x3,x4\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "'''OFFLINE 학습 방법에서는 Exploration을 최대한 다양하게 해야된다'''\n",
    "BATCH_SIZE = 30\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.5\n",
    "EPS_DECAY = 400\n",
    "TARGET_UPDATE = 10\n",
    "num_episodes = 1000\n",
    "hidden_size = 50\n",
    "tau = 0.1\n",
    "\n",
    "train_num = 20000\n",
    "\n",
    "## 메모리 크기 / Offline Learning이기 때문에 메모리의 크기가 크다.\n",
    "memory = ReplayMemory(1000000)\n",
    "\n",
    "## gym 행동 공간에서 행동의 숫자를 얻기.\n",
    "input_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "## 큰 차원의 데이터를 처리하기 위해서 NN을 사용\n",
    "policy_net = DQN(input_size,hidden_size, n_actions)\n",
    "target_net = DQN(input_size,hidden_size, n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "## 최적화 기준 RMS(Root Mean Squeare)\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr =0.001)\n",
    "\n",
    "## 몇번의 스탭을 했는지.\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Action을 하는 Q 값을 랜덤으로 설정하기 위한변수'''\n",
    "random_q = [1,2,3,4]\n",
    "\n",
    "\n",
    "## e- greedy를 이용한 Action 선택\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    ## 0과1사이의 값을 무작위로 가져옴\n",
    "    sample = random.random()\n",
    "    ## Epsilon-Threshold의 값이 step이 반복될 수록 작아진다.\n",
    "    ## 점점 greedy action을 택하게 만듬.\n",
    "    steps_done += 1\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * (steps_done) / EPS_DECAY)\n",
    "    if sample > eps_threshold :\n",
    "        with torch.no_grad():\n",
    "            \"\"\"\n",
    "            t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "            최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "            기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "            \"\"\"\n",
    "            '''4개의 Q 값 중 랜덤하게 하나를 선택하는 구조'''\n",
    "            act1,act2,act3,act4 = policy_net(Variable(state))\n",
    "            sample_num = random.choice(random_q)\n",
    "            if sample_num == 1 :\n",
    "                action = torch.FloatTensor(act1).data.max(1)[1].view(1,1)\n",
    "            elif sample_num == 2:\n",
    "                action = torch.FloatTensor(act2).data.max(1)[1].view(1,1)\n",
    "            elif sample_num == 3 :\n",
    "                action = torch.FloatTensor(act3).data.max(1)[1].view(1,1)\n",
    "            else :\n",
    "                action = torch.FloatTensor(act4).data.max(1)[1].view(1,1)\n",
    "            '''\n",
    "            ## Deterministic하게 선택하는 구조\n",
    "            act,_,_,_ = policy_net(Variable(state))\n",
    "            action = torch.FloatTensor(act).data.max(1)[1].view(1,1)\n",
    "            '''     \n",
    "            \n",
    "            \n",
    "            return action\n",
    "        # Random action 선택\n",
    "    else:\n",
    "        return torch.LongTensor([[random.randrange(n_actions)]])\n",
    "    \n",
    "    \n",
    "    '''Test 할때 사용되는 방법으로 Majority Action을 찾는다. 4개의 Q Value가 투표형식으로 Action을 선택한다.'''\n",
    "def select_majority_action(act1,act2,act3,act4):\n",
    "    ## Majority Vote\n",
    "    action1 = torch.FloatTensor(act1).data.max(1)[1].view(1,1)\n",
    "    action2 = torch.FloatTensor(act2).data.max(1)[1].view(1,1)\n",
    "    action3 = torch.FloatTensor(act3).data.max(1)[1].view(1,1)\n",
    "    action4 = torch.FloatTensor(act4).data.max(1)[1].view(1,1)\n",
    "    ## Cartpole의 Action Dim = 2 (0,1) 이므로 2가지만 생각함.\n",
    "    count_zero = 0\n",
    "    if action1 == 0 :\n",
    "        count_zero += 1\n",
    "    if action2 == 0 :\n",
    "        count_zero += 1\n",
    "    if action3 == 0 :\n",
    "        count_zero += 1\n",
    "    if action4 == 0 :\n",
    "        count_zero += 1\n",
    "\n",
    "    if count_zero >= 2 :\n",
    "        action = 0\n",
    "    else :\n",
    "        action = 1\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DATASET을 모으면서 학습하기 위한 DQN OPTIMZE MODEL  //// 굳이 사용하지 않아도 됨'''\n",
    "\n",
    "## Optimize\n",
    "def optimize_model_DQN():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    ## Memory에서 Sample을 가져옴    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    ## Batch로 나눔.\n",
    "    state_batch, action_batch, next_state_batch, reward_batch, done_batch = zip(*transitions)\n",
    "    \n",
    "    ## Sample에서 나온 Data를 각각 모음.\n",
    "    state_batch = torch.cat(state_batch)\n",
    "    next_state_batch = Variable(torch.cat(next_state_batch))\n",
    "    action_batch = Variable(torch.cat(action_batch))\n",
    "    reward_batch = Variable(torch.cat(reward_batch))\n",
    "    done_batch = Variable(torch.cat(done_batch))\n",
    "    \n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_action_values = policy_net(next_state_batch).max(1)[0]\n",
    "    ## 기대 Q 값 계산\n",
    "    expected_state_action_values = reward_batch + (GAMMA * next_state_action_values)*(1-done_batch)\n",
    "    \n",
    "\n",
    "    ## Huber 손실 계산 / L1 loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    ## 모델 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize\n",
    "'''총 4개의 Q Value로 나온 LOSS의 평균 구하기'''\n",
    "'''LOSS는 DQN 4개가 나옴'''\n",
    "'''My way  '''\n",
    "def optimize_model_EnsembleDQN1():\n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            return\n",
    "        ## Memory에서 Sample을 가져옴    \n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "        ## Batch로 나눔.\n",
    "        state_batch, action_batch, next_state_batch, reward_batch, done_batch = zip(*transitions)\n",
    "\n",
    "        ## Sample을 각각의 데이터로 모음.\n",
    "        state_batch =torch.cat(state_batch)\n",
    "        next_state_batch = Variable(torch.cat(next_state_batch))\n",
    "        action_batch = Variable(torch.cat(action_batch))\n",
    "        reward_batch = Variable(torch.cat(reward_batch))\n",
    "        done_batch = Variable(torch.cat(done_batch))\n",
    "\n",
    "\n",
    "        '''Optimize 하는 부분 / /\n",
    "        DQN 으로 Loss를 계산함'''\n",
    "\n",
    "        ## 공통적으로 사용하는 Q - Value // 4개의 Q - Value가 나옴.\n",
    "        state_action_values1,state_action_values2,\\\n",
    "        state_action_values3,state_action_values4 = policy_net(state_batch)\n",
    "\n",
    "        state_action_values1 = state_action_values1.gather(1, action_batch)\n",
    "        state_action_values2 = state_action_values2.gather(1, action_batch)\n",
    "        state_action_values3 = state_action_values3.gather(1, action_batch)\n",
    "        state_action_values4 = state_action_values4.gather(1, action_batch)\n",
    "        \n",
    "        next_state_action_values1,next_state_action_values2,next_state_action_values3,\\\n",
    "        next_state_action_values4 = policy_net(next_state_batch)\n",
    "\n",
    "        \n",
    "        '''DQN 계산하는 부분'''\n",
    "        \n",
    "        ## DQN Loss 계산\n",
    "\n",
    "        next_state_values1 = next_state_action_values1.max(1)[0]\n",
    "        next_state_values2 = next_state_action_values2.max(1)[0]\n",
    "        next_state_values3 = next_state_action_values3.max(1)[0]\n",
    "        next_state_values4 = next_state_action_values4.max(1)[0]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''첫번째 Q Value'''\n",
    "        ## 기대 Q 값 계산 \n",
    "        ## Huber 손실 계산 / L1 loss\n",
    "        dqn_expected_state_action_values1 = reward_batch + (GAMMA * next_state_values1)*(1-done_batch)\n",
    "        dqnloss1 = F.smooth_l1_loss(state_action_values1, dqn_expected_state_action_values1.unsqueeze(1))\n",
    "\n",
    "        '''두번째 Q Value'''\n",
    "        dqn_expected_state_action_values2 = reward_batch + (GAMMA * next_state_values2)*(1-done_batch)\n",
    "        dqnloss2 = F.smooth_l1_loss(state_action_values2, dqn_expected_state_action_values2.unsqueeze(1))\n",
    "        \n",
    "        '''세번째 Q Value'''\n",
    "        dqn_expected_state_action_values3 = reward_batch + (GAMMA * next_state_values3)*(1-done_batch)\n",
    "        dqnloss3 = F.smooth_l1_loss(state_action_values3, dqn_expected_state_action_values3.unsqueeze(1))\n",
    "        \n",
    "        '''네번째 Q Value'''\n",
    "        dqn_expected_state_action_values4 = reward_batch + (GAMMA * next_state_values4)*(1-done_batch)\n",
    "        dqnloss4 = F.smooth_l1_loss(state_action_values4, dqn_expected_state_action_values4.unsqueeze(1))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        ## 계산해서 나온 모든 LOSS의 평균으로 업데이트함.\n",
    "        loss = (dqnloss1+dqnloss2+dqnloss3+dqnloss4) / 4\n",
    "        ## 모델 최적화\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize\n",
    "'''총 4개의 Q Value로 나온 Q Value의 평균을 이용해서 LOSS 구하기'''\n",
    "'''LOSS는 DQN으로 한개가 나옴'''\n",
    "'''Google이 말한 방식을 코드로 표현해봄 (Modified) '''\n",
    "def optimize_model_EnsembleDQN2():\n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            return\n",
    "        ## Memory에서 Sample을 가져옴    \n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "\n",
    "        state_batch, action_batch, next_state_batch, reward_batch, done_batch = zip(*transitions)\n",
    "\n",
    "\n",
    "        state_batch =torch.cat(state_batch)\n",
    "        next_state_batch = Variable(torch.cat(next_state_batch))\n",
    "        action_batch = Variable(torch.cat(action_batch))\n",
    "        reward_batch = Variable(torch.cat(reward_batch))\n",
    "        done_batch = Variable(torch.cat(done_batch))\n",
    "\n",
    "\n",
    "        '''Optimize 하는 부분 / /\n",
    "        DQN Loss를 계산함.'''\n",
    "\n",
    "        ## 공통적으로 사용하는 Q - Value\n",
    "        state_action_values1,state_action_values2,\\\n",
    "        state_action_values3,state_action_values4 = policy_net(state_batch)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## 4개의 Q - Value의 평균을 이용해서 LOSS를 계산함.\n",
    "        mean_q_value = (state_action_values1+state_action_values2+state_action_values3+state_action_values4)/4\n",
    "\n",
    "        mean_state_action_values = mean_q_value.gather(1, action_batch)\n",
    "   \n",
    "        \n",
    "        next_state_action_values1,next_state_action_values2,next_state_action_values3,\\\n",
    "        next_state_action_values4 = policy_net(next_state_batch)\n",
    "        \n",
    "        mean_next_state_action_values = (next_state_action_values1+next_state_action_values2+\\\n",
    "                                         next_state_action_values3+next_state_action_values4)/4\n",
    "        \n",
    "        '''DQN 계산하는 부분'''\n",
    "        \n",
    "        ## DQN Loss 계산\n",
    "\n",
    "        next_state_action_values = mean_next_state_action_values.max(1)[0]\n",
    "    \n",
    "        '''평균 Q Value'''\n",
    "        ## 기대 Q 값 계산 \n",
    "        ## Huber 손실 계산 / L1 loss\n",
    "        dqn_expected_state_action_values = reward_batch + (GAMMA * next_state_action_values)*(1-done_batch)\n",
    "        dqnloss = F.smooth_l1_loss(mean_state_action_values, dqn_expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "\n",
    "\n",
    "        ## DQN에서 나온 LOSS의 평균을 이용\n",
    "        loss = dqnloss\n",
    "        \n",
    "        ## 모델 최적화\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 폴이 쓰러지면 한번에 에피소드가 종료\n",
    "## Collecting Replay(Memory)\n",
    "\n",
    "'''Offline Learning 방법'''\n",
    "'''메모리 저장하는 부분'''\n",
    "for t in count():\n",
    "    ## 환경과 상태 초기화\n",
    "    state = env.reset()\n",
    "    \n",
    "    ## 현재 상태를 지정\n",
    "    for t in count():\n",
    "        ## 행동 선택과 수행\n",
    "        state = torch.FloatTensor([state])\n",
    "        \n",
    "        action = select_action(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "               \n",
    "        if done : \n",
    "            reward = -100\n",
    "        ## 메모리에 변이 저장(한번에 에피소드에 많은 메모리가 생김)\n",
    "        memory.push((state,action, torch.FloatTensor([next_state]), \\\n",
    "                    torch.FloatTensor([reward]), torch.FloatTensor([done])))\n",
    "            \n",
    "        if done or t > 500:\n",
    "            '''DATA를 모으면서 학습하는 구조를 원한다면 학습 중인 과정을 볼수 있게\n",
    "            하는 부분'''\n",
    "            #episode_durations.appent(t+1)\n",
    "            break \n",
    "        \n",
    "        \n",
    "        ## 다음 상태로 이동\n",
    "        state = next_state\n",
    "        '''OFFLINE LEARNING 방법에서는 Episode를 통해 Dataset을 쌓으면서 학습이\n",
    "        가능하다.'''\n",
    "        '''##Data를 모으면서 학습하는 구조 : IF U WANT\n",
    "        \n",
    "        optimize_model_DQN()\n",
    "        \n",
    "    if i_episode % 100 == 0:\n",
    "        plot_durations()\n",
    "    if i_episode % 1000 == 0 :\n",
    "        clear_output()\n",
    "        \n",
    "        '''\n",
    "        \n",
    "    \n",
    "    if len(memory) >= memory.capacity :\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Section\n",
    "'''Replay에 있는 모든 데이터를 이용해서 학습한다 /\n",
    "    Optimize 하는 부분'''\n",
    "for t in range(train_num) : \n",
    "    '''두 개의 Optimize 방법 중 원하는 것을 사용하면 됨.'''\n",
    "    optimize_model_EnsembleDQN1()\n",
    "    #optimize_model_EnsembleDQN2()\n",
    "    \n",
    "    if t % TARGET_UPDATE == 0:\n",
    "        for param, target_param in zip(policy_net.parameters(),target_net.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
    "                \n",
    "                \n",
    "                \n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing Section\n",
    "results = []\n",
    "num_episodes =100\n",
    "i_episode = 0\n",
    "for i_episode in range(num_episodes):\n",
    "    ## 환경과 상태 초기화\n",
    "    state = env.reset()\n",
    "    \n",
    "    ## 에피소드 시작\n",
    "    for t in count():\n",
    "         ## 행동 선택과 수행 / Test하는 과정이므로 학습된 네트워크가 행동 결정\n",
    "        state = torch.FloatTensor([state])\n",
    "        \n",
    "        \n",
    "        act1,act2,act3,act4 = target_net(Variable(state))\n",
    "        \n",
    "        action = select_majority_action(act1,act2,act3,act4)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "               \n",
    "      \n",
    "        if done or t > 500:\n",
    "            results.append(t + 1)\n",
    "            break \n",
    "        \n",
    "        \n",
    "        ## 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "def plot_durations1():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(results, dtype=torch.float)\n",
    "    plt.title('Test')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.axhline(y=200,linestyle ='--')\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "plot_durations1()\n",
    "print('Complete')\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optimize Ensemble DQN2'''\n",
    "''' Train 1 Time'''\n",
    "plot_durations1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Optimize Ensemble DQN2'''\n",
    "''' Train 2 Times'''\n",
    "plot_durations1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Optimize Ensemble DQN1'''\n",
    "''' Train 1 Times'''\n",
    "plot_durations1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Optimize Ensemble DQN1'''\n",
    "''' Train 2 Times'''\n",
    "plot_durations1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments : 코드는 학습을 하지만, 학습 결과를 더 좋게 만드는 방법이 많이 있는 것 같다. OFFLINE LEARNING이다 보니, OFF-POLICY 방법을 사용했고, 그중에서도 ENSEMBLE DQN을 사용했다. 여러개의 Q Value를 이용한 방법이다. 확실히 데이터셋이 크고 조금만 더 수정하면 좋은 결과가 나올 것 같다. 같은 데이터셋을 통해서 OFF-Policy 알고리즘을 비교해 볼 수 있다. Ensemble Online과 마찬가지로 2개의 Optimize 방법을 만들어 보았다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
