{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "## CartPole-v0 환경을 이용, 폴이 안쓰러지게 학습\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "\n",
    "'''\n",
    "# matplotlib 설정\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "'''\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training 중인 상태를 plot으로 표현\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.axhline(y=200,linestyle ='--')\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing에서 사용\n",
    "def plot_durations1():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(results, dtype=torch.float)\n",
    "    plt.title('Test')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.axhline(y=200,linestyle ='--')\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experience Replay\n",
    "## 학습에 이용할 메모리\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(transition)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        if len(self.memory) >= self.capacity: \n",
    "            self.memory[self.position] = transition\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 큰 차원의 데이터를 다루기위해서 사용\n",
    "\n",
    "'''4개의 Q 값을 도출해내는 Network 구조를 가짐'''\n",
    "## 학습에 쓰이는  NN \n",
    "## Input은 State가 되고 Output은 Q - value가 된다.\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size*2)\n",
    "        self.fc3 = nn.Linear(hidden_size*2,action_size)\n",
    "        self.fc4 = nn.Linear(hidden_size*2,action_size)\n",
    "        self.fc5 = nn.Linear(hidden_size*2,action_size)\n",
    "        self.fc6 = nn.Linear(hidden_size*2,action_size)\n",
    "        \n",
    "    ## 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됨.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x1 = self.fc3(x)\n",
    "        x2 = self.fc4(x)\n",
    "        x3 = self.fc5(x)\n",
    "        x4 = self.fc6(x)\n",
    "        return x1,x2,x3,x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 300\n",
    "TARGET_UPDATE = 10\n",
    "num_episodes = 200\n",
    "\n",
    "hidden_size = 50\n",
    "tau = 0.1\n",
    "\n",
    "## 메모리 크기\n",
    "memory = ReplayMemory(100000)\n",
    "\n",
    "# gym 행동 공간에서 행동의 숫자를 얻기.\n",
    "input_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "## 큰 차원의 데이터를 처리하기 위해서 NN을 사용\n",
    "policy_net = DQN(input_size,hidden_size, n_actions)\n",
    "target_net = DQN(input_size,hidden_size, n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "## 최적화 기준 RMS(Root Mean Squeare)\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr =0.001)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "## 몇번의 스탭을 했는지.\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Action을 하는 Q 값을 랜덤으로 설정하기 위한변수'''\n",
    "# random_q = [1,2,3,4]\n",
    "\n",
    "'''e보다 큰 값을 통해서 Action을 선택할 때 오직 하나의 Q Value만을 이용해서 선택하는 방법을 이용'''\n",
    "## e- greedy를 이용한 Action 선택\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    ## 0과1사이의 값을 무작위로 가져옴\n",
    "    sample = random.random()\n",
    "    ## Epsilon-Threshold의 값이 step이 반복될 수록 작아진다.\n",
    "    ## 점점 greedy action을 택하게 만듬.\n",
    "    steps_done += 1\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * (steps_done) / EPS_DECAY)\n",
    "    if sample > eps_threshold :\n",
    "        with torch.no_grad():\n",
    "            \"\"\"\n",
    "            t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "            최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "            기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "            \"\"\"\n",
    "            '''4개의 Q 값 중 랜덤하게 하나를 선택하는 구조'''\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            act1,act2,act3,act4 = policy_net(Variable(state))\n",
    "            sample_num = random.choice(random_q)\n",
    "            if sample_num == 1 :\n",
    "                action = torch.FloatTensor(act1).data.max(1)[1].view(1,1)\n",
    "            elif sample_num == 2:\n",
    "                action = torch.FloatTensor(act2).data.max(1)[1].view(1,1)\n",
    "            elif sample_num == 3 :\n",
    "                action = torch.FloatTensor(act3).data.max(1)[1].view(1,1)\n",
    "            else :\n",
    "                action = torch.FloatTensor(act4).data.max(1)[1].view(1,1)\n",
    "            '''\n",
    "            ## Deterministic하게 선택하는 구조\n",
    "            act,_,_,_ = policy_net(Variable(state))\n",
    "            action = torch.FloatTensor(act).data.max(1)[1].view(1,1)\n",
    "            \n",
    "            \n",
    "            return action\n",
    "        # Random action 선택\n",
    "    else:\n",
    "        return torch.LongTensor([[random.randrange(n_actions)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Test 할때 사용되는 방법으로 Majority Action을 찾는다. 4개의 Q Value가 투표형식으로 Action을 선택한다.'''\n",
    "def select_majority_action(act1,act2,act3,act4):\n",
    "    ## Majority Vote\n",
    "    action1 = torch.FloatTensor(act1).data.max(1)[1].view(1,1)\n",
    "    action2 = torch.FloatTensor(act2).data.max(1)[1].view(1,1)\n",
    "    action3 = torch.FloatTensor(act3).data.max(1)[1].view(1,1)\n",
    "    action4 = torch.FloatTensor(act4).data.max(1)[1].view(1,1)\n",
    "    ## Cartpole의 Action Dim = 2 (0,1) 이므로 2가지만 생각함.\n",
    "    count_zero = 0\n",
    "    if action1 == 0 :\n",
    "        count_zero += 1\n",
    "    if action2 == 0 :\n",
    "        count_zero += 1\n",
    "    if action3 == 0 :\n",
    "        count_zero += 1\n",
    "    if action4 == 0 :\n",
    "        count_zero += 1\n",
    "\n",
    "    if count_zero >= 2 :\n",
    "        action = 0\n",
    "    else :\n",
    "        action = 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss 계산을 위한 함수\n",
    "def dqn_loss(q_val,next_q_val,gamma,reward,is_done) :\n",
    "    ex_q = reward + gamma * next_q_val * (1-is_done)\n",
    "    loss = F.smooth_l1_loss(q_val,ex_q.unsqueeze(1))\n",
    "    return loss\n",
    "\n",
    "def ddqn_loss(q_val,pol_next_q, targ_next_q, gamma, reward, is_done):\n",
    "    next_q = targ_next_q.gather(1,torch.max(pol_next_q,1)[1].unsqueeze(1)).squeeze(1)\n",
    "    ex_q = reward + gamma * next_q * (1-is_done)\n",
    "    loss = F.smooth_l1_loss(q_val.squeeze(),ex_q)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize\n",
    "'''총 4개의 Q Value로 나온 LOSS의 평균 구하기'''\n",
    "'''LOSS는 DQN 4개가 나옴'''\n",
    "\n",
    "def optimize_model_EnsembleDQN1():\n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            return\n",
    "        ## Memory에서 Sample을 가져옴    \n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "\n",
    "        state_batch, action_batch, next_state_batch, reward_batch, done_batch = zip(*transitions)\n",
    "\n",
    "\n",
    "        state_batch =torch.cat(state_batch)\n",
    "        next_state_batch = Variable(torch.cat(next_state_batch))\n",
    "        action_batch = Variable(torch.cat(action_batch))\n",
    "        reward_batch = Variable(torch.cat(reward_batch))\n",
    "        done_batch = Variable(torch.cat(done_batch))\n",
    "\n",
    "\n",
    "        '''Optimize 하는 부분 / /\n",
    "        DQN Loss를 계산함.'''\n",
    "\n",
    "        ## 공통적으로 사용하는 Q - Value\n",
    "        state_action_values1,state_action_values2,\\\n",
    "        state_action_values3,state_action_values4 = policy_net(state_batch)\n",
    "\n",
    "        state_action_values1 = state_action_values1.gather(1, action_batch)\n",
    "        state_action_values2 = state_action_values2.gather(1, action_batch)\n",
    "        state_action_values3 = state_action_values3.gather(1, action_batch)\n",
    "        state_action_values4 = state_action_values4.gather(1, action_batch)\n",
    "        \n",
    "        next_state_action_values1,next_state_action_values2,next_state_action_values3,\\\n",
    "        next_state_action_values4 = policy_net(next_state_batch)\n",
    "\n",
    "        \n",
    "        '''DQN 계산하는 부분'''\n",
    "        \n",
    "        ## DQN Loss 계산\n",
    "\n",
    "        next_state_values1 = next_state_action_values1.max(1)[0]\n",
    "        next_state_values2 = next_state_action_values2.max(1)[0]\n",
    "        next_state_values3 = next_state_action_values3.max(1)[0]\n",
    "        next_state_values4 = next_state_action_values4.max(1)[0]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''첫번째 Q Value'''\n",
    "        ## 기대 Q 값 계산 \n",
    "        ## Huber 손실 계산 / L1 loss\n",
    "        \n",
    "        dqnloss1 = dqn_loss(state_action_values1,next_state_values1,GAMMA,reward_batch,done_batch)\n",
    "                \n",
    "        '''두번째 Q Value'''\n",
    "        dqnloss2 = dqn_loss(state_action_values2,next_state_values2,GAMMA,reward_batch,done_batch)\n",
    "        \n",
    "        '''세번째 Q Value'''\n",
    "        dqnloss3 = dqn_loss(state_action_values3,next_state_values3,GAMMA,reward_batch,done_batch)\n",
    "        \n",
    "        '''네번째 Q Value'''\n",
    "        dqnloss4 = dqn_loss(state_action_values4,next_state_values4,GAMMA,reward_batch,done_batch)\n",
    "        \n",
    "\n",
    "\n",
    "        ## 4개의 Q Value를 이용해서 Double 총 4개의 LOSS의 평균을 구함\n",
    "        loss = (dqnloss1+dqnloss2+dqnloss3+dqnloss4) / 4\n",
    "        \n",
    "        ## 모델 최적화\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize\n",
    "'''총 4개의 Q Value로 나온 Q Value의 평균을 이용해서 LOSS 구하기'''\n",
    "'''LOSS는 DQN으로 나옴'''\n",
    "\n",
    "def optimize_model_EnsembleDQN2():\n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            return\n",
    "        ## Memory에서 Sample을 가져옴    \n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "\n",
    "        state_batch, action_batch, next_state_batch, reward_batch, done_batch = zip(*transitions)\n",
    "\n",
    "\n",
    "        state_batch =torch.cat(state_batch)\n",
    "        next_state_batch = Variable(torch.cat(next_state_batch))\n",
    "        action_batch = Variable(torch.cat(action_batch))\n",
    "        reward_batch = Variable(torch.cat(reward_batch))\n",
    "        done_batch = Variable(torch.cat(done_batch))\n",
    "\n",
    "\n",
    "        '''Optimize 하는 부분 / /\n",
    "        DQN Loss를 계산함.'''\n",
    "\n",
    "        ## 공통적으로 사용하는 Q - Value\n",
    "        state_action_values1,state_action_values2,\\\n",
    "        state_action_values3,state_action_values4 = policy_net(state_batch)\n",
    "        \n",
    "        mean_q_value = (state_action_values1+state_action_values2+state_action_values3+state_action_values4)/4\n",
    "        \n",
    "        mean_state_action_values = mean_q_value.gather(1, action_batch)\n",
    "   \n",
    "        \n",
    "        next_state_action_values1,next_state_action_values2,next_state_action_values3,\\\n",
    "        next_state_action_values4 = policy_net(next_state_batch)\n",
    "        \n",
    "        mean_next_state_action_values = (next_state_action_values1+next_state_action_values2+\\\n",
    "                                         next_state_action_values3+next_state_action_values4)/4\n",
    "        \n",
    "        '''DQN 계산하는 부분'''\n",
    "        \n",
    "        ## DQN Loss 계산\n",
    "        \n",
    "        next_state_action_values = mean_next_state_action_values.max(1)[0]\n",
    "    \n",
    "        '''평균 Q Value'''\n",
    "        ## 기대 Q 값 계산 \n",
    "        ## Huber 손실 계산 / L1 loss\n",
    "        dqnloss = dqn_loss(mean_state_action_values,next_state_action_values,GAMMA,reward_batch,done_batch)\n",
    "        \n",
    "       \n",
    "        loss = dqnloss\n",
    "\n",
    "        ## 모델 최적화\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 폴이 쓰러지면 한번에 에피소드가 종료\n",
    "## Training Section\n",
    "## 이 부분을 여러번 돌리면 학습을 여러번 한 효과\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    ## 환경과 상태 초기화\n",
    "    state = env.reset()\n",
    "    \n",
    "    ## 현재 상태를 지정\n",
    "    for t in count():\n",
    "        ## 행동 선택과 수행\n",
    "        state = torch.FloatTensor([state])\n",
    "\n",
    "        action = select_action(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "               \n",
    "        if done : \n",
    "            reward = -100\n",
    "        ## 메모리에 변이 저장(한번에 에피소드에 많은 메모리가 생김)\n",
    "        memory.push((state,action, torch.FloatTensor([next_state]), \\\n",
    "                    torch.FloatTensor([reward]), torch.FloatTensor([done])))\n",
    "\n",
    "        if done or t > 500:\n",
    "            episode_durations.append(t + 1)\n",
    "            break \n",
    "        \n",
    "        \n",
    "        ## 다음 상태로 이동\n",
    "        state = next_state\n",
    "        \n",
    "        ## 최적화 한단계 수행(Policy 네트워크에서)\n",
    "        ## 원하는 OPTIMIZE MODEL을 이용하면 됨.\n",
    "        optimize_model_EnsembleDQN1()\n",
    "        #optimize_model_EnsembleDQN2()\n",
    "\n",
    "        \n",
    "    ## Policy 네트워크 업데이트, 모든 웨이트와 바이어스 복사 => Target으로\n",
    "    ## Target_update 기준으로 몇번에 한번씩 업데이트 할 것인지 결정\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        for param, target_param in zip(policy_net.parameters(),target_net.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
    "    ## Output을 계속 볼수 있게\n",
    "    if i_episode % 200 ==0:\n",
    "        plot_durations()\n",
    "    \n",
    "    if i_episode % 500 == 0:\n",
    "        clear_output()\n",
    "    \n",
    "    \n",
    "plot_durations()\n",
    "\n",
    "print('Complete')\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3496832ff6ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_majority_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-91abda04ac7d>\u001b[0m in \u001b[0;36mselect_majority_action\u001b[0;34m(act1, act2, act3, act4)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m## Majority Vote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0maction3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maction4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Testing Section\n",
    "results = []\n",
    "num_episode =200\n",
    "i_episode = 0\n",
    "for i_episode in range(num_episode):\n",
    "    ## 환경과 상태 초기화\n",
    "    state = env.reset()\n",
    "\n",
    "    ## 에피소드 시작\n",
    "    for t in count():\n",
    "        ## 행동 선택과 수행 / Test하는 과정이므로 학습된 네트워크가 행동 결정\n",
    "        state = torch.FloatTensor([state]) \n",
    "        \n",
    "        act1,act2,act3,act4 = target_net(Variable(state))\n",
    "        \n",
    "        action = select_majority_action(act1,act2,act3,act4)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "\n",
    "        if done or t > 500:\n",
    "            results.append(t + 1)\n",
    "            break \n",
    "\n",
    "        ## 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "plot_durations\n",
    "plot_durations1()\n",
    "print('Complete')\n",
    "\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Train for 200 Times Using Method 2\"\n",
    "plot_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Test for 200 Times Using Method 2\"\n",
    "plot_durations1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Train for 200 Times Using Method 1\"\n",
    "plot_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Test for 200 Times Using Method 1\"\n",
    "plot_durations1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments : Ensemble 방법은 하나의 큰 알고리즘을 이용하는 것보다 여러 개의 작은 알고리즘을 이용해서 성능을 높이는 방법인데, (Q -learning) 일반적인 DQN과 Double DQN을 같이 사용하면 오히려 성능이 좋지 않고, Double DQN이나 DQN만으로 Ensemble하는 것이 성능이 좋다. (Why?) 또 Optimize 방법을 두 가지로 나누어서 실험해 보았다. 하나는 4개의 Q - Value의 평균을 구하고 Loss를 계산하는 방식이고 또 다른 하나는 4개의 Q - Value로 각각 Loss를 계산한 후에 Loss의 평균을 구하는 방식이다.Optimize 1 방식이 더 결과가 좋다 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
