{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Testing for STATIC DATASET RL IS IT WORK??????'''\n",
    "'''RL for Classification'''\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Bernoulli\n",
    "from torch.autograd import Variable\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "## Credit Card Data를 이용\n",
    "train = pd.read_csv(\"./uci_creditcard-train-0.0-0.0.csv\")\n",
    "test = pd.read_csv(\"./uci_creditcard-test-0.0-0.0.csv\")\n",
    "\n",
    "train = train.to_numpy()\n",
    "test = test.to_numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''열별로 데이터 전처리하는 과정 '''\n",
    "def z_score_normalize(data) :\n",
    "    normal_data = data\n",
    "    for columns in range(data.shape[1]-2) :\n",
    "        normal_data[:][columns] = (data[:][columns] - np.mean(data[:][columns])) /\\\n",
    "                                    np.std(data[:][columns])\n",
    "\n",
    "    return normal_data\n",
    "\n",
    "\n",
    "normalized_train = z_score_normalize(train)\n",
    "normalized_test = z_score_normalize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "            super(actor_network,self).__init__()\n",
    "            self.fc1 = nn.Linear(state_dim,128)\n",
    "            self.fc2 = nn.Linear(128,32)\n",
    "            self.fc3 = nn.Linear(32,action_dim)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        ## Action의 범위 생각해야해\n",
    "        return x\n",
    "\n",
    "class critic_network(nn.Module):\n",
    "    ## 근데 여기서 state\n",
    "    def __init__(self,state_dim):\n",
    "        super(critic_network,self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128,32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter\n",
    "eps = 0.2\n",
    "gamma = 0.99\n",
    "train_num = 3500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "## Input data size는 Column의 개수 -1 마지막 두개의 열은 Classification과 Valide Set을 구분.\n",
    "state_dim = train.shape[1]-3\n",
    "## Classification 이므로 Action은 2가 나옴.\n",
    "action_dim = 2\n",
    "\n",
    "actor = actor_network()\n",
    "critic = critic_network(state_dim)\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(),lr = 0.0001)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr = 0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reward를 받을 때 데이터셋의 비율이 다르므로 Reward도 다르게 적용\n",
    "def get_reward(action,target) :\n",
    "    if action == target :\n",
    "        accuracy = 1\n",
    "        if action == 1 : \n",
    "            reward = 23996 / 5356\n",
    "        elif action == 0 :\n",
    "            reward = 23996 / 18640\n",
    "    else :\n",
    "        accuracy = 0\n",
    "        if action == 1 :\n",
    "            reward = -23996 / 5356\n",
    "        elif action == 0 :\n",
    "            reward = - 23996 / 18640\n",
    "    \n",
    "    return reward ,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(old_log_prob, log_prob, advantage, eps):\n",
    "    ratio = (log_prob - old_log_prob).exp()\n",
    "    clipped = torch.clamp(ratio, 1-eps, 1+eps)*advantage\n",
    "    \n",
    "    m = torch.min(ratio*advantage, clipped)\n",
    "    return -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations(reward):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    reward = torch.FloatTensor(reward)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Accuracy')\n",
    "    #plt.axhline(y=90,linestyle ='--')\n",
    "    plt.plot(reward.numpy())\n",
    "    if len(reward) >= 100:\n",
    "        means = reward.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy = []\n",
    "\n",
    "\n",
    "## Traing set을 이용한 학습 \n",
    "##\n",
    "for i_num in range(train_num):\n",
    "    ## Batch size 크기 만큼의 Trajectory가 만들어짐.\n",
    "    trajectory = normalized_train[np.random.choice(normalized_train.shape[0],BATCH_SIZE,replace =False),:]\n",
    "    total_acc = 0\n",
    "    ## Trajectory를 만듬 길이는 BATCH_SIZE\n",
    "    ## Gradient Descent\n",
    "\n",
    "    loss = 0\n",
    "    total_loss = 0\n",
    "    ## Trajectory를 따라가면서 학습\n",
    "    for t in range(BATCH_SIZE-1) :      \n",
    "        ## state를 tensor로 바꿔서 이용\n",
    "        state = trajectory[t][1:24]\n",
    "        state = torch.FloatTensor(state)\n",
    "        state = Variable(state)\n",
    "        ## Classification Target\n",
    "        target = trajectory[t][24]\n",
    "        \n",
    "        next_state = trajectory[t+1][1:24]\n",
    "        next_state = torch.FloatTensor(state)\n",
    "        next_state = Variable(next_state)\n",
    "        \n",
    "        \n",
    "        ## Action을 선택하기 위해 이항분포 => Discrete Action space\n",
    "        probs = actor(state)\n",
    "        m = Bernoulli(probs)\n",
    "        action = m.sample()\n",
    "        prob_action = m.log_prob(action)\n",
    "        action = action.data.numpy().astype(int)[0]\n",
    "       \n",
    "        reward,accuracy= get_reward(action,target) \n",
    "        adv = reward + gamma * critic(next_state) - critic(state)\n",
    "        \n",
    "        \n",
    "        ## With Discout reward\n",
    "        \n",
    "     \n",
    "        total_acc += accuracy\n",
    "    \n",
    "        if t > 1 :\n",
    "            actor_loss = policy_loss(prev_prob_act.detach(),prob_action,adv.detach(),eps)\n",
    "            actor_loss = torch.mean(actor_loss)\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            critic_loss = adv.pow(2).mean()\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            \n",
    "        prev_prob_act = prob_action\n",
    "        \n",
    "    total_accuracy.append(total_acc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (i_num+1) % 10 == 0 :\n",
    "        print('Current_train_num : ',i_num, \"Actor loss : \",actor_loss.item(), \\\n",
    "                             \"Critic loss : \",critic_loss.item())\n",
    "    \n",
    "    if (i_num+1) % 100 == 0 :\n",
    "        plot_durations(total_accuracy)\n",
    "    if (i_num+1) % 500 ==0 :\n",
    "        clear_output()\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_accuracy = []\n",
    "test_num = 6004 ## Test set의 행의 크기\n",
    "total_acc = 0\n",
    "\n",
    "##Test set을 이용한 확인 \n",
    "##\n",
    "for i_num in range(test_num):\n",
    "    \n",
    "    state = normalized_test[i_num][1:24]\n",
    "    \n",
    "    ## state를 tensor로 바꿔서 이용\n",
    "    state = torch.FloatTensor(state)\n",
    "    state = Variable(state)\n",
    "    ## Classification Target\n",
    "    target = normalized_test[i_num][24]\n",
    "\n",
    "    ## Action을 선택하기 위해 이항분포 => Discrete Action space\n",
    "    probs = actor(state)\n",
    "    m = Bernoulli(probs = probs)\n",
    "    action = m.sample()\n",
    "    action = action.data.numpy().astype(int)[0]\n",
    "    _,accuracy= get_reward(action,target)\n",
    "\n",
    "    total_acc += accuracy   \n",
    "    total_accuracy.append(total_acc)\n",
    "    if (i_num+1) % 10 == 0 :\n",
    "        print('Current_test_num : ',i_num, \"Accuracy : \",total_acc / i_num)\n",
    "\n",
    "    if (i_num+1) % 100 == 0 :\n",
    "        plot_durations(total_accuracy)\n",
    "    if (i_num+1) % 500 ==0 :\n",
    "        clear_output()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULT (Correct / Total Test NUM) :    0.7837747792770281\n",
      "Total Test Num =  6004\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL RESULT (Correct / Total Test NUM) :   \", total_acc / i_num  )\n",
    "print(\"Total Test Num = \", test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments : PPO를 이용해서 Classification 문제를 해결하려고 시도해보았다. PG 보다 성능은 높게 나올거 같았지만 비슷했다. Reward Function에서 문제가 있는 것 같기도하다. Reward를 2배로 설정해봤지만 결과는 비슷했다.결과값을 거의 항상 0으로 뱉기때문인것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
